{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kunlib import KUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Kernel(nn.Module):\n",
    "    def __init__(self, input_dim, input_len, \n",
    "                 output_dim, output_len, params={}):\n",
    "        super(Kernel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.input_len = input_len\n",
    "        self.output_dim = output_dim\n",
    "        self.output_len = output_len \n",
    "        self.params = params \n",
    "\n",
    "        self.is_in_encoder = False #input_len >= output_len\n",
    "        self.is_in_decoder = False #input_len >= output_len\n",
    "\n",
    "    def update_params(self, params):\n",
    "        # Iterate over all fields in the class and update if they exist in params\n",
    "        self.params = params \n",
    "        for key, value in params.items():\n",
    "            setattr(self, key, value)  # Set the attribute from params\n",
    "    \n",
    "class KernelWrapper(nn.Module):\n",
    "    def __init__(self, kernal, input_dim, input_len, \n",
    "                 output_dim=1, output_len=1, \n",
    "                 num_hidden_layers=1,  \n",
    "                 mode=\"concate\", params={},verbose=False):\n",
    "        super(KernelWrapper, self).__init__()\n",
    "\n",
    "        # kernal : kernal(input_dim, input_len, output_dim, output_len)\n",
    "        assert (issubclass(kernal, Kernel) or isinstance(kernal, nn.Module))\n",
    "        if isinstance(kernal, nn.Module) : \n",
    "          print(f\"kernel {kernal} heiritated nn.Module may not adapt.\")\n",
    "\n",
    "        self.input_dim, self.input_len, self.output_dim, self.output_len = \\\n",
    "                        input_dim, input_len, output_dim, output_len\n",
    "        self.verbose = verbose\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.hidden_size_list = []\n",
    "\n",
    "        if issubclass(kernal, Kernel):\n",
    "            print(\"kernal \", kernal, \"is a Kernel\")\n",
    "            self.kernel = kernal(input_dim, input_len, output_dim, output_len, params=params)\n",
    "        else:\n",
    "            assert False, f\"kernal {kernal} is not recognized\"\n",
    "            \n",
    "        self._unet_skip_output = None\n",
    "        self._unet_skip_input = None\n",
    "\n",
    "        #self.linear_unet_skip_input = nn.Linear(self.input_dim, self.input_dim)\n",
    "\n",
    "        self.transpose = False\n",
    "        self.concat = True if mode == \"concate\" else False\n",
    "        self.concat_mode = mode\n",
    "\n",
    "    def f(self, x):\n",
    "        if self.verbose : \n",
    "          print(\"---KernelWrapper.f(x) Input x.shape: \", x.shape)\n",
    "        x = self.kernel(x)\n",
    "        if self.verbose : \n",
    "          print(\"---KernelWrapper.f(x) Output x.shape: \", x.shape)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, train=False):\n",
    "        if self.verbose : \n",
    "          print(\"---KernelWrapper.forward(x) Input x.shape:\", x.shape)\n",
    "          print(\"---train:\", train)\n",
    "          if self._unet_skip_input is not None:\n",
    "              print(\"---_unet_skip_input.shape\", self._unet_skip_input.shape)\n",
    "          else:\n",
    "              print(\"---_unet_skip_input\", self._unet_skip_input)\n",
    "\n",
    "        if self.transpose and self._unet_skip_input is not None:\n",
    "            if self.verbose : \n",
    "                print(\"self.transpose and self._unet_skip_input\")\n",
    "                print(\"--x.shape\", x.shape)\n",
    "            if np.prod(x.shape) == np.prod(self._unet_skip_input.shape):\n",
    "                if self.verbose : \n",
    "                    print( self.concat, x.shape)\n",
    "                if self.concat:\n",
    "                    x = torch.cat([x, self._unet_skip_input.reshape(x.shape)], dim=-1)\n",
    "                    #print( \"after, \", self.concat, x.shape)\n",
    "                else:\n",
    "                    x = x + self._unet_skip_input.reshape(x.shape)\n",
    "                #print(\"_unet_skip_input.shape\", self._unet_skip_input.shape)\n",
    "            #x[len(self._unet_skip_input):] = x[len(self._unet_skip_input):] + self._unet_skip_input\n",
    "        #x = x.transpose(1, 2)   # # (batch, d_model , lag) to (batch, lag, d_model)\n",
    "        else:\n",
    "            pass\n",
    "        if self.verbose : \n",
    "            print(\"reshape - > x.shape\", x.shape)\n",
    "        x = x.reshape(-1, self.input_len, self.input_dim)\n",
    "\n",
    "        if self.verbose : \n",
    "            print(\"after reshape - > x.shape\", x.shape)\n",
    "        x = self.f(x)\n",
    "\n",
    "        if self.verbose : \n",
    "            print(\"after x = self.f(x) - > x.shape\", x.shape)\n",
    "        assert x.shape[1] == self.output_len and x.shape[2] == self.output_dim\n",
    "\n",
    "        if not self.transpose:\n",
    "          self._unet_skip_output = x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear MLP Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Kernel):\n",
    "    def __init__(self, input_dim, input_len, \n",
    "                 output_dim, output_len, params={}):\n",
    "        super(Linear, self).__init__(input_dim, input_len, \n",
    "                 output_dim, output_len)\n",
    "        # declear parameters\n",
    "        self.activation = \"tanh\"\n",
    "        self.drop_out_p = 0.05\n",
    "        self.num_hidden_layers = 0\n",
    "        self.update_params(params=params)\n",
    "\n",
    "        # compute input and output size\n",
    "        self.in_size = input_len*input_dim\n",
    "        self.out_size = output_len*output_dim\n",
    "\n",
    "        # prepare layers\n",
    "        self.layers = []\n",
    "\n",
    "        # check in encoder or decoder \n",
    "        self.is_in_encoder = (self.input_len >= self.output_len)\n",
    "        self.is_in_decoder = not self.is_in_encoder\n",
    "\n",
    "        # in encoder\n",
    "        if self.is_in_encoder:\n",
    "          gap = int((self.in_size - self.out_size) / (self.num_hidden_layers + 1))\n",
    "          self.hidden_size_list = [self.in_size - i * gap for i in range(1, self.num_hidden_layers + 1)]\n",
    "        \n",
    "        # in decoder\n",
    "        else:\n",
    "          gap = int((self.out_size - self.in_size) / (self.num_hidden_layers + 1))\n",
    "          self.hidden_size_list = [self.in_size + i * gap for i in range(1, self.num_hidden_layers + 1)]\n",
    "        # add linear layers\n",
    "        for i in range(self.num_hidden_layers):\n",
    "            self.layers.append(nn.Linear(self.in_size, self.hidden_size_list[i]))\n",
    "\n",
    "            if self.activation.lower() == \"relu\":\n",
    "                self.layers.append(nn.ReLU())\n",
    "            elif self.activation.lower() == \"tanh\": \n",
    "                self.layers.append(nn.Tanh())\n",
    "\n",
    "            self.layers.append(nn.Dropout(self.drop_out_p))\n",
    "            self.in_size = self.hidden_size_list[i]\n",
    "\n",
    "        self.layers.append(nn.Linear(self.in_size, self.out_size))\n",
    "\n",
    "        self.layers = nn.Sequential(* self.layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.reshape(-1, self.input_len * self.input_dim)\n",
    "        #print(\"x.shape,\", x.shape)\n",
    "        x = self.layers(x)\n",
    "        x = x.reshape(-1, self.output_len, self.output_dim)\n",
    "        #print(\"x.shape,\", x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal  <class '__main__.Linear'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): Linear(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.01, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=384, bias=True)\n",
      "      (4): Tanh()\n",
      "      (5): Dropout(p=0.01, inplace=False)\n",
      "      (6): Linear(in_features=384, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(Linear, input_dim=128, input_len=1, \n",
    "                 output_dim=128, output_len=4, \n",
    "                 num_hidden_layers=1, \n",
    "                 mode=\"concate\", params={\"num_hidden_layers\":2, \n",
    "                                         \"drop_out_p\":0.05, \n",
    "                                         \"activation\":\"tanh\"}, \n",
    "                                verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "x = kw(torch.rand((4,1,128)))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(Kernel):\n",
    "    def __init__(self, input_dim, input_len, \n",
    "                 output_dim, output_len, params={}):\n",
    "        super(LSTM, self).__init__(input_dim, input_len, \n",
    "                 output_dim, output_len)\n",
    "        # declear parameters\n",
    "        self.drop_out_p = 0.05\n",
    "        self.num_hidden_layers = 0\n",
    "        self.update_params(params=params)\n",
    "\n",
    "\n",
    "        self.lstm_dim = max(input_dim, output_dim)\n",
    "        self.lstm_len = max(input_len, output_len)\n",
    "\n",
    "        # compute input and output size\n",
    "        self.in_size = input_len*input_dim\n",
    "        self.out_size = output_len*output_dim\n",
    "        self.lstm_size = self.lstm_dim*self.lstm_len\n",
    "\n",
    "        # prepare layers\n",
    "        self.layers = []\n",
    "\n",
    "        # check in encoder or decoder \n",
    "        self.is_in_encoder = (self.input_len >= self.output_len)\n",
    "        self.is_in_decoder = not self.is_in_encoder\n",
    "\n",
    "        # Define the LSTM and Linear layers\n",
    "        self.linear_projection_in = nn.Linear(self.in_size, self.lstm_size)\n",
    "        self.linear_projection_out = nn.Linear(self.lstm_size, self.out_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(self.lstm_dim, self.lstm_dim, \n",
    "                            self.num_hidden_layers, dropout=self.drop_out_p, \n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LSTM. If we are in encoder mode, process the input sequence through LSTM\n",
    "        and use the last hidden state to compute the final output using the linear layer.\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.in_size)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_projection_in(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.lstm_len, self.lstm_dim)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        x, (h_n, c_n) = self.lstm(x)  # x, lstm_out contains all hidden states, h_n is the last hidden state\n",
    "\n",
    "        # Use the last hidden state (h_n) for linear transformation\n",
    "        #print(x.shape, h_n.shape, c_n.shape)\n",
    "        # Apply linear transformation\n",
    "        x = x.reshape(-1, self.lstm_size)\n",
    "        x = self.linear_projection_out(x)\n",
    "        x = x.reshape(-1, self.output_len, self.output_dim)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal  <class '__main__.LSTM'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): LSTM(\n",
      "    (linear_projection_in): Linear(in_features=4, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lstm): LSTM(128, 128, batch_first=True, dropout=0.05)\n",
      "  )\n",
      ")\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jyou\\AppData\\Local\\Continuum\\anaconda3\\envs\\Base\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(LSTM, input_dim=1, input_len=4, \n",
    "                 output_dim=128, output_len=1, \n",
    "                 num_hidden_layers=1, \n",
    "                 mode=\"add\", params={\"num_hidden_layers\":1, \n",
    "                                         \"drop_out_p\":0.05,\n",
    "                                         \"activation\":\"tanh\"}, \n",
    "                                verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 4, 1])\n",
      "torch.Size([13, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,4,1))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(Kernel):\n",
    "    def __init__(self, input_dim, input_len, \n",
    "                 output_dim, output_len, params={}):\n",
    "        super(RNN, self).__init__(input_dim, input_len, \n",
    "                 output_dim, output_len)\n",
    "        # declear parameters\n",
    "        self.drop_out_p = 0.05\n",
    "        self.num_hidden_layers = 0\n",
    "        self.update_params(params=params)\n",
    "\n",
    "\n",
    "        self.lstm_dim = max(input_dim, output_dim)\n",
    "        self.lstm_len = max(input_len, output_len)\n",
    "\n",
    "        # compute input and output size\n",
    "        self.in_size = input_len*input_dim\n",
    "        self.out_size = output_len*output_dim\n",
    "        self.lstm_size = self.lstm_dim*self.lstm_len\n",
    "\n",
    "        # prepare layers\n",
    "        self.layers = []\n",
    "\n",
    "        # check in encoder or decoder \n",
    "        self.is_in_encoder = (self.input_len >= self.output_len)\n",
    "        self.is_in_decoder = not self.is_in_encoder\n",
    "\n",
    "        # Define the LSTM and Linear layers\n",
    "        self.linear_projection_in = nn.Linear(self.in_size, self.lstm_size)\n",
    "        self.linear_projection_out = nn.Linear(self.lstm_size, self.out_size)\n",
    "\n",
    "        self.lstm = nn.RNN(self.lstm_dim, self.lstm_dim, \n",
    "                            self.num_hidden_layers, dropout=self.drop_out_p, \n",
    "                            batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LSTM. If we are in encoder mode, process the input sequence through LSTM\n",
    "        and use the last hidden state to compute the final output using the linear layer.\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.in_size)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_projection_in(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.lstm_len, self.lstm_dim)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        x, _ = self.lstm(x)  # x, lstm_out contains all hidden states, h_n is the last hidden state\n",
    "\n",
    "        # Use the last hidden state (h_n) for linear transformation\n",
    "        #print(x.shape, h_n.shape, c_n.shape)\n",
    "        # Apply linear transformation\n",
    "        x = x.reshape(-1, self.lstm_size)\n",
    "        x = self.linear_projection_out(x)\n",
    "        x = x.reshape(-1, self.output_len, self.output_dim)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal  <class '__main__.RNN'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): RNN(\n",
      "    (linear_projection_in): Linear(in_features=4, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lstm): RNN(128, 128, batch_first=True, dropout=0.05)\n",
      "  )\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(RNN, input_dim=1, input_len=4, \n",
    "                 output_dim=128, output_len=1, \n",
    "                 num_hidden_layers=1, \n",
    "                 mode=\"add\", params={\"num_hidden_layers\":1, \n",
    "                                         \"drop_out_p\":0.05,\n",
    "                                         \"activation\":\"tanh\"}, \n",
    "                                verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 4, 1])\n",
      "torch.Size([13, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,4,1))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % self.num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.depth = d_model // self.num_heads\n",
    "        \n",
    "        # Linear layers for queries, keys, and values\n",
    "        self.Wq = nn.Linear(d_model, d_model)\n",
    "        self.Wk = nn.Linear(d_model, d_model)\n",
    "        self.Wv = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Output linear layer\n",
    "        self.fc = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        Q = self.Wq(x).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        K = self.Wk(x).view(batch_size, -1, self.num_heads, self.depth)\n",
    "        V = self.Wv(x).view(batch_size, -1, self.num_heads, self.depth)\n",
    "\n",
    "        # Permute to bring num_heads dimension to second position\n",
    "        Q = Q.permute(0, 2, 1, 3)\n",
    "        K = K.permute(0, 2, 1, 3)\n",
    "        V = V.permute(0, 2, 1, 3)\n",
    "\n",
    "        # Scaled dot-product attention\n",
    "        scores = torch.matmul(Q, K.permute(0, 1, 3, 2)) / math.sqrt(self.depth)\n",
    "        \n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        attention = F.softmax(scores, dim=-1)\n",
    "\n",
    "        # Weighted sum of value vectors\n",
    "        out = torch.matmul(attention, V)\n",
    "        out = out.permute(0, 2, 1, 3).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # Final linear transformation\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=500):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        if d_model % 2 == 1:\n",
    "            d_model_1 = d_model + 1\n",
    "        else:\n",
    "            d_model_1 = d_model\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model_1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model_1, 2).float() * (-math.log(10000.0) / d_model_1))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe_const', pe[:, :d_model])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe_const[:x.size(1), :].unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "class AttentionBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(AttentionBlock, self).__init__()\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.relu = nn.LeakyReLU()\n",
    "        self.linear = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # First residual connection\n",
    "        residual = x\n",
    "        x = self.multi_head_attention(x, mask)\n",
    "        x = self.relu(x) + residual\n",
    "\n",
    "        # Second residual connection\n",
    "        residual = x\n",
    "        x = self.linear(x)\n",
    "        x = x + residual\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(Kernel):\n",
    "    def __init__(self, input_dim, input_len, \n",
    "                 output_dim, output_len, params={}):\n",
    "        super(Transformer, self).__init__(input_dim, input_len, \n",
    "                 output_dim, output_len)\n",
    "        # declear parameters\n",
    "        self.drop_out_p = 0.05\n",
    "        self.num_hidden_layers = 0\n",
    "        self.num_heads = 2\n",
    "        self.update_params(params=params)\n",
    "\n",
    "\n",
    "        self.lstm_dim = max(input_dim, output_dim)\n",
    "        self.lstm_len = max(input_len, output_len)\n",
    "\n",
    "        # compute input and output size\n",
    "        self.in_size = input_len*input_dim\n",
    "        self.out_size = output_len*output_dim\n",
    "        self.lstm_size = self.lstm_dim*self.lstm_len\n",
    "\n",
    "        # prepare layers\n",
    "        self.layers = []\n",
    "\n",
    "        # check in encoder or decoder \n",
    "        self.is_in_encoder = (self.input_len >= self.output_len)\n",
    "        self.is_in_decoder = not self.is_in_encoder\n",
    "\n",
    "        # Define the LSTM and Linear layers\n",
    "        self.linear_projection_in = nn.Linear(self.in_size, self.lstm_size)\n",
    "        self.linear_projection_out = nn.Linear(self.lstm_size, self.out_size)\n",
    "\n",
    "        self.attention = nn.Sequential(*[\n",
    "                                AttentionBlock(d_model=self.lstm_dim, \n",
    "                                num_heads=self.num_heads) for i in range(self.num_hidden_layers)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass for LSTM. If we are in encoder mode, process the input sequence through LSTM\n",
    "        and use the last hidden state to compute the final output using the linear layer.\n",
    "        \"\"\"\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.in_size)\n",
    "        #print(x.shape)\n",
    "        x = self.linear_projection_in(x)\n",
    "        #print(x.shape)\n",
    "        x = x.reshape(-1, self.lstm_len, self.lstm_dim)\n",
    "        #print(x.shape)\n",
    "\n",
    "        # Pass through LSTM\n",
    "        x = self.attention(x)  # x, lstm_out contains all hidden states, h_n is the last hidden state\n",
    "\n",
    "        # Use the last hidden state (h_n) for linear transformation\n",
    "        #print(x.shape, h_n.shape, c_n.shape)\n",
    "        # Apply linear transformation\n",
    "        x = x.reshape(-1, self.lstm_size)\n",
    "        x = self.linear_projection_out(x)\n",
    "        x = x.reshape(-1, self.output_len, self.output_dim)\n",
    "        #print(x.shape)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernal  <class '__main__.Transformer'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): Transformer(\n",
      "    (linear_projection_in): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention): Sequential(\n",
      "      (0): AttentionBlock(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (relu): LeakyReLU(negative_slope=0.01)\n",
      "        (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(Transformer, input_dim=128, input_len=1, \n",
    "                 output_dim=128, output_len=4, \n",
    "                 num_hidden_layers=1, \n",
    "                 mode=\"add\", params={\"num_hidden_layers\":1, \n",
    "                                         \"drop_out_p\":0.05,\n",
    "                                         \"activation\":\"tanh\"}, \n",
    "                                verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 128])\n",
      "torch.Size([13, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,1,128))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kunlib_v2 import KUNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KUNet(input_dim=128, input_len, \n",
    "                 n_width=1, n_height=1, \n",
    "                 output_dim=1, output_len=1, \n",
    "                 hidden_dim=20,  \n",
    "                 kernal_model=nn.Linear, \n",
    "                 params={} verbose=False):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kun = KUNet(input_dim=1, input_len=8, \n",
    "                 n_width=[1], n_height=[8,8], \n",
    "                 latent_dim=128, latent_len=1, \n",
    "                 output_dim=1, output_len=8, \n",
    "                 hidden_dim=128, num_hidden_layers=0, \n",
    "                 kernel=[Linear, LSTM, Transformer], non_linear_kernel_pos='011',\n",
    "                 skip_conn=True, skip_mode=\"concat\",\n",
    "                 inverse_norm=False, mean_norm=True,\n",
    "                 chanel_independent=True, residual = True, verbose=False)\n",
    "\n",
    "print(kun)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
