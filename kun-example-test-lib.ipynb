{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kunlib_v2 import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear MLP Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): Linear(\n",
      "    (layers): Sequential(\n",
      "      (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Dropout(p=0.05, inplace=False)\n",
      "      (3): Linear(in_features=320, out_features=512, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(Linear, input_dim=128, input_len=1, \n",
    "                 output_dim=128, output_len=4, \n",
    "                 kernel_hidden_layer=1, \n",
    "                 params={\"kernel_hidden_layer\":2, \n",
    "                            \"drop_out_p\":0.05, \n",
    "                            \"activation\":\"tanh\"}, \n",
    "                            verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "x = kw(torch.rand((4,1,128)))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'torch.nn.modules.linear.Linear'> is a nn.Linear Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): Linear(in_features=512, out_features=128, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(nn.Linear, input_dim=128, input_len=4, \n",
    "                 output_dim=128, output_len=1, \n",
    "                 kernel_hidden_layer=1, \n",
    "                 params={\"kernel_hidden_layer\":2, \n",
    "                            \"drop_out_p\":0.05, \n",
    "                            \"activation\":\"tanh\"}, \n",
    "                            verbose=False)\n",
    "print(kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = kw(torch.rand((4,4,128)))\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.LSTM'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): LSTM(\n",
      "    (linear_projection_in): Linear(in_features=4, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lstm): LSTM(128, 128, batch_first=True, dropout=0.05)\n",
      "  )\n",
      ")\n",
      "True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jyou\\AppData\\Local\\Continuum\\anaconda3\\envs\\Base\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:83: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.05 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(LSTM, input_dim=1, input_len=4, \n",
    "                 output_dim=128, output_len=1, \n",
    "                 kernel_hidden_layer=1, \n",
    "                 params={\"kernel_hidden_layer\":1, \n",
    "                            \"drop_out_p\":0.05,\n",
    "                            \"activation\":\"tanh\"}, \n",
    "                            verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 4, 1])\n",
      "torch.Size([13, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,4,1))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.RNN'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): RNN(\n",
      "    (linear_projection_in): Linear(in_features=4, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=128, bias=True)\n",
      "    (lstm): RNN(128, 128, batch_first=True, dropout=0.05)\n",
      "  )\n",
      ")\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(RNN, input_dim=1, input_len=4, \n",
    "                 output_dim=128, output_len=1, \n",
    "                 kernel_hidden_layer=1, \n",
    "                 params={\"kernel_hidden_layer\":1, \n",
    "                            \"drop_out_p\":0.05,\n",
    "                            \"activation\":\"tanh\"}, \n",
    "                            verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 4, 1])\n",
      "torch.Size([13, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,4,1))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.Transformer'> is a Kernel\n",
      "KernelWrapper(\n",
      "  (kernel): Transformer(\n",
      "    (linear_projection_in): Linear(in_features=128, out_features=512, bias=True)\n",
      "    (linear_projection_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (attention): Sequential(\n",
      "      (0): AttentionBlock(\n",
      "        (multi_head_attention): MultiHeadAttention(\n",
      "          (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "          (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (relu): LeakyReLU(negative_slope=0.01)\n",
      "        (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "kw = KernelWrapper(Transformer, input_dim=128, input_len=1, \n",
    "                 output_dim=128, output_len=4, \n",
    "                 kernel_hidden_layer=1, \n",
    "                 params={\"kernel_hidden_layer\":1, \n",
    "                            \"drop_out_p\":0.05,\n",
    "                            \"activation\":\"tanh\"}, \n",
    "                            verbose=False)\n",
    "print(kw)\n",
    "print(kw.kernel.is_in_encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([13, 1, 128])\n",
      "torch.Size([13, 4, 128])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(size=(13,1,128))\n",
    "print(x.shape)\n",
    "x = kw(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "KUNetEncoder(\n",
      "  (layers): Sequential(\n",
      "    (0): KernelWrapper(\n",
      "      (kernel): Linear(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=320, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.05, inplace=False)\n",
      "          (3): Linear(in_features=320, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): KernelWrapper(\n",
      "      (kernel): Linear(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=320, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.05, inplace=False)\n",
      "          (3): Linear(in_features=320, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): KernelWrapper(\n",
      "      (kernel): Linear(\n",
      "        (layers): Sequential(\n",
      "          (0): Linear(in_features=512, out_features=320, bias=True)\n",
      "          (1): Tanh()\n",
      "          (2): Dropout(p=0.05, inplace=False)\n",
      "          (3): Linear(in_features=320, out_features=128, bias=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([13, 64, 128])\n",
      "torch.Size([13, 1, 128])\n"
     ]
    }
   ],
   "source": [
    "kun_encoder = KUNetEncoder(input_dim=128, input_len=4, \n",
    "                 n_width=[1], n_height=[4, 4], \n",
    "                 output_dim=128, output_len=1, \n",
    "                 hidden_dim=[128,128,128], kernel_hidden_layer=[1,1,1], \n",
    "                 kernel=[Linear]*3, verbose=False, params={})\n",
    "print(kun_encoder)\n",
    "x = torch.rand(size=(13,64,128))\n",
    "print(x.shape)\n",
    "x = kun_encoder(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "Sequential(\n",
      "  (0): KernelWrapper(\n",
      "    (kernel): Linear(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.05, inplace=False)\n",
      "        (3): Linear(in_features=320, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): KernelWrapper(\n",
      "    (kernel): Linear(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.05, inplace=False)\n",
      "        (3): Linear(in_features=320, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (2): KernelWrapper(\n",
      "    (kernel): Linear(\n",
      "      (layers): Sequential(\n",
      "        (0): Linear(in_features=128, out_features=320, bias=True)\n",
      "        (1): Tanh()\n",
      "        (2): Dropout(p=0.05, inplace=False)\n",
      "        (3): Linear(in_features=320, out_features=512, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "torch.Size([13, 1, 128])\n",
      "-KUN-Decoder.forward(x) start x.shape  torch.Size([13, 1, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([13, 1, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([13, 1, 128])\n",
      "after reshape - > x.shape torch.Size([13, 1, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([13, 1, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([13, 4, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([13, 4, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([13, 4, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([13, 4, 128])\n",
      "after reshape - > x.shape torch.Size([52, 1, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([52, 1, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([52, 4, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([52, 4, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([52, 4, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([52, 4, 128])\n",
      "after reshape - > x.shape torch.Size([208, 1, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([208, 1, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([208, 4, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([208, 4, 128])\n",
      "-KUN-Decoder.forward(x) self.layers(x).shape  torch.Size([208, 4, 128])\n",
      "-KUN-Decoder.forward(x) x.reshape((-1,)+ tuple(self.n_width) + (1,) + tuple(self.n_height) + (self.output_dim,)  + (self.output_len,)).reshape  torch.Size([13, 1, 1, 16, 4, 128])\n",
      "-KUN-Decoder.forward(x) x.transpose(2+len(self.n_width), 2+len(self.n_width)+2+len(self.n_height)).shape  torch.Size([13, 16, 1, 1, 4, 128])\n",
      "-KUN-Decoder.forward(x) x.reshape((x_shape[0], self.total_width, self.total_height*self.input_len)).shape  torch.Size([13, 64, 128])\n",
      "torch.Size([13, 64, 128])\n"
     ]
    }
   ],
   "source": [
    "kun_decoder = KUNetDecoder(input_dim=128, input_len=1, \n",
    "                 n_width=[1], n_height=[4, 4], \n",
    "                 output_dim=128, output_len=4, \n",
    "                 hidden_dim=[128,128,128], kernel_hidden_layer=[1,1,1], \n",
    "                 kernel=[Linear]*3, verbose=True, params={\"unet_skip\":True, \n",
    "                 \"unet_skip_concat\":False,\n",
    "                 })\n",
    "print(kun_decoder.layers)\n",
    "x = torch.rand(size=(13,1,128))\n",
    "print(x.shape)\n",
    "x = kun_decoder(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "self.lag_list [8, 8, 8]\n",
      "hidden_dim [128, 128, 128]\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.LSTM'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Transformer'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Transformer'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.LSTM'> is a Kernel\n",
      "kernel  <class 'kunlib_v2.Linear'> is a Kernel\n",
      "KUNet(\n",
      "  (model): KUNetEncoderDecoder(\n",
      "    (encoder): KUNetEncoder(\n",
      "      (layers): Sequential(\n",
      "        (0): KernelWrapper(\n",
      "          (kernel): Linear(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=1024, out_features=576, bias=True)\n",
      "              (1): Tanh()\n",
      "              (2): Dropout(p=0.05, inplace=False)\n",
      "              (3): Linear(in_features=576, out_features=128, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): KernelWrapper(\n",
      "          (kernel): LSTM(\n",
      "            (linear_projection_in): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (linear_projection_out): Linear(in_features=1024, out_features=128, bias=True)\n",
      "            (lstm): LSTM(128, 128, batch_first=True, dropout=0.05)\n",
      "          )\n",
      "        )\n",
      "        (2): KernelWrapper(\n",
      "          (kernel): Transformer(\n",
      "            (linear_projection_in): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (linear_projection_out): Linear(in_features=1024, out_features=128, bias=True)\n",
      "            (attention): Sequential(\n",
      "              (0): AttentionBlock(\n",
      "                (multi_head_attention): MultiHeadAttention(\n",
      "                  (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "                )\n",
      "                (relu): LeakyReLU(negative_slope=0.01)\n",
      "                (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (decoder): KUNetDecoder(\n",
      "      (layers): Sequential(\n",
      "        (0): KernelWrapper(\n",
      "          (kernel): Transformer(\n",
      "            (linear_projection_in): Linear(in_features=128, out_features=1024, bias=True)\n",
      "            (linear_projection_out): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (attention): Sequential(\n",
      "              (0): AttentionBlock(\n",
      "                (multi_head_attention): MultiHeadAttention(\n",
      "                  (Wq): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (Wk): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (Wv): Linear(in_features=128, out_features=128, bias=True)\n",
      "                  (fc): Linear(in_features=128, out_features=128, bias=True)\n",
      "                )\n",
      "                (relu): LeakyReLU(negative_slope=0.01)\n",
      "                (linear): Linear(in_features=128, out_features=128, bias=True)\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): KernelWrapper(\n",
      "          (kernel): LSTM(\n",
      "            (linear_projection_in): Linear(in_features=256, out_features=2048, bias=True)\n",
      "            (linear_projection_out): Linear(in_features=2048, out_features=1024, bias=True)\n",
      "            (lstm): LSTM(256, 256, batch_first=True, dropout=0.05)\n",
      "          )\n",
      "        )\n",
      "        (2): KernelWrapper(\n",
      "          (kernel): Linear(\n",
      "            (layers): Sequential(\n",
      "              (0): Linear(in_features=256, out_features=640, bias=True)\n",
      "              (1): Tanh()\n",
      "              (2): Dropout(p=0.05, inplace=False)\n",
      "              (3): Linear(in_features=640, out_features=1024, bias=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "kun = KUNet(input_dim=128, input_len=8, \n",
    "                 n_width=[1], n_height=[8, 8], \n",
    "                 latent_dim=128, latent_len=1, \n",
    "                 output_dim=128, output_len=8, \n",
    "                 hidden_dim=[128]*3, \n",
    "                 kernel=[Linear, LSTM, Transformer], kernel_hidden_layer=[1, 1, 1],\n",
    "                 verbose=True, params={\"unet_skip\":True, \n",
    "                         \"unet_skip_concat\":True,\n",
    "\n",
    "                         \"inverse_norm\":False,\n",
    "                         \"mean_norm\":False,\n",
    "                         \"chanel_independent\":False,\n",
    "                         \"residual\":False, })\n",
    "print(kun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-KUN-Encoder.forward(x) Input x.shape:  torch.Size([13, 512, 128])\n",
      "-KUN-Encoder.forward(x) x = x.reshape((-1,) + tuple(self.n_width) + (self.input_dim,) + tuple(self.n_height) + (1,) +(self.input_len,)).shape  torch.Size([13, 64, 8, 1, 1, 128])\n",
      "-KUN-Encoder.forward(x)  x = x.transpose(1+len(self.n_width), 1+len(self.n_width)+len(self.n_height)+1).shape  torch.Size([13, 1, 64, 8, 128])\n",
      "-KUN-Encoder.forward(x) x = x.reshape((-1, self.input_len, self.input_dim)).shape  torch.Size([832, 8, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([832, 8, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([832, 8, 128])\n",
      "after reshape - > x.shape torch.Size([832, 8, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([832, 8, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([832, 1, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([832, 1, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([832, 1, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([832, 1, 128])\n",
      "after reshape - > x.shape torch.Size([104, 8, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([104, 8, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([104, 1, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([104, 1, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([104, 1, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([104, 1, 128])\n",
      "after reshape - > x.shape torch.Size([13, 8, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([13, 8, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([13, 1, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([13, 1, 128])\n",
      "-KUN-Encoder.forward(x) self.layers(x).shape  torch.Size([13, 1, 128])\n",
      "-KUN-Encoder.forward(x) x = x.reshape((-1, self.output_len, self.output_dim)).shape  torch.Size([13, 1, 128])\n",
      "-KUN-Decoder.forward(x) start x.shape  torch.Size([13, 1, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([13, 1, 128])\n",
      "---train: False\n",
      "---_unet_skip_input None\n",
      "reshape - > x.shape torch.Size([13, 1, 128])\n",
      "after reshape - > x.shape torch.Size([13, 1, 128])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([13, 1, 128])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([13, 8, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([13, 8, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([13, 8, 128])\n",
      "---train: False\n",
      "---_unet_skip_input.shape torch.Size([104, 1, 128])\n",
      "self.transpose and self._unet_skip_input\n",
      "--x.shape torch.Size([13, 8, 128])\n",
      "self.unet_skip_concat, x.shape True torch.Size([13, 8, 128])\n",
      "reshape - > x.shape torch.Size([13, 8, 256])\n",
      "after reshape - > x.shape torch.Size([104, 1, 256])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([104, 1, 256])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([104, 8, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([104, 8, 128])\n",
      "---KernelWrapper.forward(x) Input x.shape: torch.Size([104, 8, 128])\n",
      "---train: False\n",
      "---_unet_skip_input.shape torch.Size([832, 1, 128])\n",
      "self.transpose and self._unet_skip_input\n",
      "--x.shape torch.Size([104, 8, 128])\n",
      "self.unet_skip_concat, x.shape True torch.Size([104, 8, 128])\n",
      "reshape - > x.shape torch.Size([104, 8, 256])\n",
      "after reshape - > x.shape torch.Size([832, 1, 256])\n",
      "---KernelWrapper.f(x) Input x.shape:  torch.Size([832, 1, 256])\n",
      "---KernelWrapper.f(x) Output x.shape:  torch.Size([832, 8, 128])\n",
      "after x = self.f(x) - > x.shape torch.Size([832, 8, 128])\n",
      "-KUN-Decoder.forward(x) self.layers(x).shape  torch.Size([832, 8, 128])\n",
      "-KUN-Decoder.forward(x) x.reshape((-1,)+ tuple(self.n_width) + (1,) + tuple(self.n_height) + (self.output_dim,)  + (self.output_len,)).reshape  torch.Size([13, 1, 1, 64, 8, 128])\n",
      "-KUN-Decoder.forward(x) x.transpose(2+len(self.n_width), 2+len(self.n_width)+2+len(self.n_height)).shape  torch.Size([13, 64, 1, 1, 8, 128])\n",
      "-KUN-Decoder.forward(x) x.reshape((x_shape[0], self.total_width, self.total_height*self.input_len)).shape  torch.Size([13, 512, 128])\n",
      "torch.Size([13, 512, 128])\n"
     ]
    }
   ],
   "source": [
    "x = kun(torch.rand((13,512,128)))\n",
    "print(x.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
